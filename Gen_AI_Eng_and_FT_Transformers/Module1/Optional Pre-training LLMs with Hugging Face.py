#!/usr/bin/env python
# coding: utf-8

# <p style="text-align:center">
#     <a href="https://skills.network" target="_blank">
#     <img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png" width="200" alt="Skills Network Logo"  />
#     </a>
# </p>
# 

# # **Pre-training LLMs with Hugging Face**
# 
# Estimated time: **45** minutes
# 
# # Introduction
# 
# This project aims to introduce you to the process of pretraining large language models (LLMs) using the popular Hugging Face library. Hugging Face is a leading open-source platform for natural language processing that provides a wide range of pretrained models and tools for fine-tuning and deploying these models.
# 
# You will learn how to load pre-trained models from Hugging Face and make inferences using the Pipeline module. Additionally, you will learn how to further train pre-trained LLMs on your own data (self-supervised fine-tuning). By the end of this lab, you will have a solid understanding of how to pretrain LLMs and store them to later fine-tune for your specific use cases. This will empower you to create powerful and customized natural language processing solutions.
# 

# # __Table of Contents__
# 
# <ol>
#     <li><a href="#Objectives">Objectives</a></li>
#     <li>
#         <a href="#Setup">Setup</a>
#         <ol>
#             <li><a href="#Installing-required-libraries">Installing required libraries</a></li>
#             <li><a href="#Importing-required-libraries">Importing required libraries</a></li>
#         </ol>
#     </li>
#     <li><a href="#Pretraining-and-self-supervised-fine-tuning">Pretraining and self-supervised fine-tuning</a>
#         <ol>
#             <li><a href="#Importing-required-datasets">Importing required datasets</a></li>
#             <li><a href="#Loading-the-saved-model">Loading the saved model</a></li>
#             <li><a href="#Inferencing-a-pretrained-BERT-model">Inferencing a pretrained BERT model</a></li>
#         </ol>
#     </li>
#     <li><a href="#Exercise">Exercise</a></li>
# </ol>
# 

# ---
# 

# # Objectives
# 
# After completing this lab, you will be able to:
# 
# 
#  - Load pretrained LLMs from Hugging Face and make inferences using the pipeline module
#  - Train pretrained LLMs on your data 
#  - Store LLMs to fine-tune them for specific use cases
#  
# 

# ---
# 

# # Setup
# 

# ### Installing required libraries
# The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip` in the code cell below:
# 
# _PS: To run lab this in your own environment, please note that the versions of libraries may differ due to dependencies._
# 

# In[6]:


# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.
# !pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch=2.1.0+cu118
# - Update a specific package
# !pip install pmdarima -U
# - Update a package to specific version
# !pip install --upgrade pmdarima==2.0.2
# Note: If your environment doesn't support "!pip install", use "!mamba install"


# The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:
# 

# In[7]:


#!pip install transformers==4.40.0 
#get_ipython().system('pip install -U git+https://github.com/huggingface/transformers')
#get_ipython().system('pip install datasets # 2.15.0')
#get_ipython().system('pip install portalocker>=2/0.0')
#get_ipython().system('pip install -q -U git+https://github.com/huggingface/accelerate.git')
#get_ipython().system('pip install torch==2.3.0')
#get_ipython().system('pip install -U torchvision')
#get_ipython().system('pip install protobuf==3.20.*')


# ### Importing required libraries
# 
# _It is recommended that you import all required libraries in one place (here):_
# * Note: if you got an error after running the cell below, try restarting the Kernel as some packages need a restart to be effective.
# 

# In[1]:


#get_ipython().system('pip install --user datasets #AV')
#get_ipython().system('pip install --upgrade --user datasets transformers #AV')
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments
from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling
from transformers import pipeline
from datasets import load_dataset

from tqdm.auto import tqdm
import math
import time
import os


# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


# Disable tokenizer parallelism to avoid deadlocks.
# 

# In[2]:


# Set the environment variable TOKENIZERS_PARALLELISM to 'false'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'


# ---
# 

# # Pretraining and self-supervised fine-tuning
# 

# Pretraining is a technique used in natural language processing (NLP) to train large language models (LLMs) on a vast corpus of unlabeled text data. The goal is to capture the general patterns and semantic relationships present in natural language, allowing the model to develop a deep understanding of language structure and meaning.
# 
# The motivation behind pretraining transformers is to address the limitations of traditional NLP approaches that often require significant amounts of labeled data for each specific task. By leveraging the abundance of unlabeled text data, pretraining enables the model to learn fundamental language skills through self-supervised objectives, facilitating transfer learning.
# 
# The pretraining objectives, such as masked language modeling (MLM) and next sentence prediction (NSP), play a crucial role in the success of transformer models. Pretrained models can be further tuned by training them on domain-specific unlabeled data, which is known as self-supervised fine-tuning.
# 
# Also, the model can be fine-tuned on specific downstream tasks using labeled data, a process known as supervised fine-tuning, further improving its performance.
# 
# In the following sections of this lab, you will explore pretraining objectives, loading pretrained models, data preparation, and the fine-tuning process. By the end, you will have a solid understanding of pretraining and self-supervised fine-tuning, empowering you to apply these techniques to solve real-world NLP problems.
# 

# Let's start with loading a pretrained model from Hugging Face and making an inference:
# 

# In[3]:


model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

pipe = pipeline("text-generation", model=model,tokenizer=tokenizer)
print(pipe("This movie was really")[0]["generated_text"])


# ## Pre-training Objectives
# 
# Pre-training objectives are crucial components of the pre-training process for transformers. These objectives define the tasks that the model is trained on during the pre-training phase, allowing it to learn meaningful contextual representations of language. Three commonly used pre-training objectives are masked language modeling (MLM), next sentence prediction (NSP) and next Ttoken prediction.
# 
# 1. Masked Language Modeling (MLM):
#    Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.
# 
# 2. Next Sentence Prediction (NSP):
#    Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.
# 
# 3. Next Token Prediction:
#     In this objective, the model is trained to predict the next token in a sequence of text. The model is presented with a sequence of text and must learn to predict the most likely next token based on the context.
# 
# It's important to note that different pre-trained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.
# 

# ## Self-supervised training of a BERT model
# Training a BERT(Bidirectional Encoder Representations from Transformers) model is a complex and time-consuming process that requires a large corpus of unlabeled text data and significant computational resources. However, we provide you with a simplified exercise to demonstrate the steps involved in pre-training a BERT model using the Masked Language Modeling (MLM) objective.
# 
# For this exercise, we'll use the Hugging Face Transformers library, which provides pre-implemented BERT models and tools for pre-training. You will be instructed to:
# - Prepare the train dataset
# - Train a Tokenizer
# - Preprocess the dataset
# - Pre-train BERT using an MLM task
# - Evaluate the trained model
# 

# ### Importing required datasets
# 
# The WikiText dataset is a widely used benchmark dataset in the field of natural language processing (NLP). The dataset contains a large amount of text extracted from Wikipedia, which is a vast online encyclopedia covering a wide range of topics. The articles in the WikiText dataset are preprocessed to remove formatting, hyperlinks, and other metadata, resulting in a clean text corpus.
# 
# The WikiText dataset has 4 different configs, and is divided into three parts: a training set, a validation set, and a test set. The training set is used for training language models, while the validation and test sets are used for evaluating the performance of the models.
# First, let's load the datasets and concatenate them together to create a big dataset.
# 
# *Note: The original BERT was pretrained on Wikipedia and BookCorpus datasets.
# 

# In[4]:


# Load the datasets
print("load dataset")
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")


# Let's check the dataset:
# 

# In[5]:


print(dataset)


# check a sample record
# 

# In[6]:


#check a sample record
dataset["train"][400]


# This dataset contains 36,718 rows of training data. If you do not run the code on a GPU-powered notebook, you will need to decrease the size of dataset to be able to complete the training. You can uncomment the commands below to select a desired section of dataset:
# 

# In[7]:


#dataset["train"] = dataset["train"].select([i for i in range(1000)])
#dataset["test"] = dataset["test"].select([i for i in range(200)])


# Below files are next used in creating TextDataset objects for the training:
# 

# In[8]:


# Path to save the datasets to text files
output_file_train = "wikitext_dataset_train.txt"
output_file_test = "wikitext_dataset_test.txt"

# Open the output file in write mode
with open(output_file_train, "w", encoding="utf-8") as f:
    # Iterate over each example in the dataset
    for example in dataset["train"]:
        # Write the example text to the file
        f.write(example["text"] + "\n")

# Open the output file in write mode
with open(output_file_test, "w", encoding="utf-8") as f:
    # Iterate over each example in the dataset
    for example in dataset["test"]:
        # Write the example text to the file
        f.write(example["text"] + "\n")


# You need to define a tokenizer to be used for tokenizing the dataset.
# 

# In[9]:


# create a tokenizer from existing one to re-use special tokens
bert_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")


# In[10]:


model_name = 'bert-base-uncased'

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)


# ### Training a Tokenizer(Optional)
# 
# In the previous cell, you created an instance of tokenizer from a pre-trained BERT tokenizer. If you want to train the tokenizer on your own dataset, you can uncomment the code below. This is specially helpful when using transformers for specific areas such as medicine where tokens are somehow different than the general tokens that tokenizers are created based on. (You can skip this step if you do not want to train the tokenizer on your specific data):
# 

# In[12]:


## create a python generator to dynamically load the data
def batch_iterator(batch_size=10000):
    for i in tqdm(range(0, len(dataset), batch_size)):
        yield dataset['train'][i : i + batch_size]["text"]

## create a tokenizer from existing one to re-use special tokens
bert_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

## train the tokenizer using our own dataset
bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)


# ### Pretraining
# 
# In this step, we define the configuration of the BERT model and create the model:
# #### Define the BERT Configuration
# Here, we define the configuration settings for a BERT model using `BertConfig`. This includes setting various parameters related to the model's architecture:
# - **vocab_size=30522**: Specifies the size of the vocabulary. This number should match the vocabulary size used by the tokenizer.
# - **hidden_size=768**: Sets the size of the hidden layers.
# - **num_hidden_layers=12**: Determines the number of hidden layers in the transformer model.
# - **num_attention_heads=12**: Sets the number of attention heads in each attention layer.
# - **intermediate_size=3072**: Specifies the size of the "intermediate" (i.e., feed-forward) layer within the transformer.
# 
# 

# In[13]:

print("Pretraining")
# Define the BERT configuration
config = BertConfig(
    vocab_size=len(bert_tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)
    hidden_size=768,  # Set the hidden size
    num_hidden_layers=12,  # Set the number of layers
    num_attention_heads=12,  # Set the number of attention heads
    intermediate_size=3072,  # Set the intermediate size
)


#  Create the BERT model for pre-training:
# 

# In[14]:


# Create the BERT model for pre-training
model = BertForMaskedLM(config)


# check model configuration
# 

# In[15]:


# check model configuration
model


# ### Define the Training Dataset
# Here, we define a training dataset using the `TextDataset` class, which is suited for loading and processing text data for training language models. This setup typically involves a few key parameters:
# 
# - **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used. Here, `bert_tokenizer` is an instance of a BERT tokenizer, responsible for converting text into tokens that the model can understand.
# - **file_path="wikitext_dataset_train.txt"**: The path to the pre-training data file. This should point to a text file containing the training data.
# - **block_size=128**: Sets the desired block size for training. This defines the length of the sequences that the model will be trained on
# 
# The `TextDataset` class is designed to take large pieces of text (such as those found in the specified file), tokenize them, and efficiently handle them in manageable blocks of the specified size.
# 
# 

# In[16]:


# Prepare the pre-training data as a TextDataset
train_dataset = TextDataset(
    tokenizer=bert_tokenizer,
    file_path="wikitext_dataset_train.txt",  # Path to your pre-training data file
    block_size=128  # Set the desired block size for training
)
test_dataset = TextDataset(
    tokenizer=bert_tokenizer,
    file_path="wikitext_dataset_test.txt",  # Path to your pre-training data file
    block_size=128  # Set the desired block size for training
)


# examining  one sample the token indexes  are shown here with the block size.
# 

# In[17]:


train_dataset[0]


# Then, we prepare data for the MLM task (masking random tokens):
# ### Define the Data Collator for Language Modeling
# This line of code sets up a `DataCollatorForLanguageModeling` from the Hugging Face Transformers library. A data collator is used during training to dynamically create batches of data. For language modeling, particularly for models like BERT that use masked language modeling (MLM), this collator prepares training batches by automatically masking tokens according to a specified probability. Here are the details of the parameters used:
# 
# - **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used with the data collator. The `bert_tokenizer` is responsible for tokenizing the text and converting it to the format expected by the model.
# - **mlm=True**: Indicates that the data collator should mask tokens for masked language modeling training. This parameter being set to `True` configures the collator to randomly mask some of the tokens in the input data, which the model will then attempt to predict.
# - **mlm_probability=0.15**: Sets the probability with which tokens will be masked. A probability of 0.15 means that, on average, 15% of the tokens in any sequence will be replaced with a mask token.
# 

# In[18]:


# Prepare the data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15
)


# In[19]:


# check how collator transforms a sample input data record
data_collator([train_dataset[0]])


# Now, we train the BERT Model using the Trainer module. (For a complete list of training arguments, check [here](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.TrainingArguments)):
# This section configures the training process by specifying various parameters that control how the model is trained, evaluated, and saved:
# 
# - **output_dir="./trained_model"**: Specifies the directory where the trained model and other output files will be saved.
# - **overwrite_output_dir=True**: If set to `True`, this will overwrite the contents of the output directory if it already exists. This is useful when running experiments multiple times.
# - **do_eval=True**: Enables evaluation of the model. If `True`, the model will be evaluated at the specified intervals.
# - **evaluation_strategy="epoch"**: Defines when the model should be evaluated. Setting this to "epoch" means the model will be evaluated at the end of each epoch.
# - **learning_rate=5e-5**: Sets the learning rate for training the model. This is a typical learning rate for fine-tuning BERT-like models.
# - **num_train_epochs=10**: Specifies the number of training epochs. Each epoch involves a full pass over the training data.
# - **per_device_train_batch_size=2**: Sets the batch size for training on each device. This should be set based on the memory capacity of your hardware.
# - **save_total_limit=2**: Limits the total number of model checkpoints to be saved. Only the most recent two checkpoints will be kept.
# - **logging_steps=20**: Determines how often to log training information, which can help monitor the training process.
# 

# In[20]:


'''# Define the training arguments
training_args = TrainingArguments(
    output_dir="./trained_model",  # Specify the output directory for the trained model
    overwrite_output_dir=True,
    do_eval=True,
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    num_train_epochs=10,  # Specify the number of training epochs
    per_device_train_batch_size=2,  # Set the batch size for training
    save_total_limit=2,  # Limit the total number of saved checkpoints
    logging_steps = 20
    
)

# Instantiate the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Start the pre-training
trainer.train()'''


# ## Evaluating Model Performance
# 
# Let's check the performance of the trained model. Perplexity is commonly used to compare different language models or different configurations of the same model.
# After training, perplexity can be calculated on a held-out evaluation dataset to assess the model's performance. The perplexity is calculated by feeding the evaluation dataset through the model and comparing the predicted probabilities of the target tokens with the actual token values that are masked.
# 
# A lower perplexity score indicates that the model has a better understanding of the language and is more effective at predicting the masked tokens. It suggests that the model has learned useful representations and can generalize well to unseen data.
# 

# In[21]:


'''eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")'''


# ## Loading the saved model
# If you want to skip training and load the model that you trained for 10 epochs, go ahead and uncomment the following cell:
# 

# In[24]:


import torch #AV
#get_ipython().system("wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'")
import os
import requests
if os.path.exists('bert-scratch-model.pt') :
    print("Le fichier bert-scratch-model.pt existe")
else :
    url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'
    response = requests.get(url)

    with open('bert-scratch-model.pt', 'wb') as f:
        f.write(response.content)
model.load_state_dict(torch.load('bert-scratch-model.pt',map_location=torch.device('cpu')))


# The simplest way to try out the model for inference is to use it in a pipeline(). Instantiate a pipeline for fill-mask with your model, and pass your text to it. If you like, you can use the top_k parameter to specify how many predictions to return:
# 

# In[25]:


# Define the input text with a masked token
text = "This is a [MASK] movie!"
print(text)

# Create a pipeline for the "fill-mask" task
mask_filler = pipeline("fill-mask", model=model,tokenizer=bert_tokenizer)

# Generate predictions by filling the mask in the input text
results = mask_filler(text) #top_k parameter can be set 

# Print the predicted sequences
for result in results:
    print(f"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}")


# You can see that [MASK] is replaced by the most frequent token. This weak performance can be due to insufficient training, lack of training data, model architecture, or not tuning hyperparameters. Let's try a pretrained model from Hugging Face:
# 

# ## Inferencing a pretrained BERT model
# 

# In[26]:


# Load the pretrained BERT model and tokenizer
pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

# Define the input text with a masked token
text = "This is a [MASK] movie!"

# Create the pipeline
mask_filler = pipeline(task='fill-mask', model=pretrained_model,tokenizer=pretrained_tokenizer)

# Perform inference using the pipeline
results = mask_filler(text)
for result in results:
    print(f"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}")


# This pretrianed model performs way better than the model you just trained for a few epochs using a single dataset. Still, pretrained models cannot be used for specific tasks, such as sentiment extraction or sequence classification. This is why supervised fine-tuning methods are introduced.
# 

# ---
# 

# ## Exercise
# 

# 1. Create a model and tokenizer using Hugging Face library.
# 2. Go to this [link](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending)
# 3. Choose a Text Classification dataset that you can load, for instance 'stanfordnlp/snli'
# 4. Use that dataset to train your model(please be mindful of the resources available for the training) and evaluate it.
# 
#    >Note: The lab environment doesn't have the resources to support the training and this might cause the kernel to die.
# 

# <details><summary>Click here for a hint</summary>
# 
# -   SNLI has 3 labels
# -   You can use `load_dataset("stanfordnlp/snli")` to load the dataset
# </details>
# 

# <details><summary>Click here for the solution</summary>
# 
# ```python
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# from datasets import load_dataset
# 
# # Load the SNLI dataset
# snli = load_dataset("stanfordnlp/snli")
# 
# # Preprocessing function
# def preprocess_function(examples):
#   premise = examples["premise"]
#   hypothesis = examples["hypothesis"]
#   return tokenizer(premise, hypothesis, padding="max_length", truncation=True)
# 
# model_name = "bert-base-uncased"
# 
# # Load tokenizer and model
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)
# 
# # Apply preprocessing to training and validation sets
# train_encoded = snli["train"].map(preprocess_function, batched=True)
# val_encoded = snli["validation"].map(preprocess_function, batched=True)
# 
# # Training function (replace with your training loop)
# from transformers import TrainingArguments, Trainer
# 
# training_args = TrainingArguments(
#     output_dir="./results",  # Replace with your output directory
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=3,
# )
# 
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_encoded,
#     eval_dataset=val_encoded,
# )
# 
# trainer.train()
# 
# # Evaluation function (replace with your metrics)
# from sklearn.metrics import accuracy_score
# 
# predictions, labels = trainer.predict(val_encoded)
# accuracy = accuracy_score(labels, predictions.argmax(-1))
# print(f"Accuracy on validation set: {accuracy:.4f}")
# 
# ```
# 
# </details>
# 

# In[ ]:


from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset

# Load the SNLI dataset
snli = load_dataset("stanfordnlp/snli")

# Preprocessing function
def preprocess_function(examples):
  premise = examples["premise"]
  hypothesis = examples["hypothesis"]
  return tokenizer(premise, hypothesis, padding="max_length", truncation=True)

model_name = "bert-base-uncased"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Apply preprocessing to training and validation sets
train_encoded = snli["train"].map(preprocess_function, batched=True)
val_encoded = snli["validation"].map(preprocess_function, batched=True)

# Training function (replace with your training loop)
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",  # Replace with your output directory
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=1,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encoded,
    eval_dataset=val_encoded,
)

# too long !
# trainer.train()

# Evaluation function (replace with your metrics)
''' too long
import sklearn
from sklearn.metrics import accuracy_score

predictions, labels = trainer.predict(val_encoded)
accuracy = accuracy_score(labels, predictions.argmax(-1))
print(f"Accuracy on validation set: {accuracy:.4f}")
'''

# # Congratulations! You have completed the lab
# 

# ## Authors
# 

# [Fateme Akbari](https://author.skills.network/instructors/fateme_akbari)
# 

# Â© Copyright IBM Corporation. All rights reserved.
# 
