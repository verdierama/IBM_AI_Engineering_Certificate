{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f551afe494c148ff8405477a4c775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77285a547ffd4377a9eb093bf6d37574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 10:02:56.421877: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-01-28 10:02:56.428822: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394310000 Hz\n",
      "2025-01-28 10:02:56.429426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a7efc0b560 executing computations on platform Host. Devices:\n",
      "2025-01-28 10:02:56.429468: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2025-01-28 10:02:56.455699: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fed34e6ba10>,\n",
       " <keras.layers.core.Dense at 0x7fed268d3d90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fedb2212e50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fedaa4cb4d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fedaa4cba50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fedaa4cbc90>,\n",
       " <keras.layers.core.Activation at 0x7fedf015c3d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fedabb04b50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fedab9cbe90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fedab945050>,\n",
       " <keras.layers.core.Activation at 0x7fedac4c3cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fedab96d590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fedab8c44d0>,\n",
       " <keras.layers.core.Activation at 0x7fedab8c4c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7feda8483b50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7feda8389c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7feda839a150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7feda82ff4d0>,\n",
       " <keras.layers.merge.Add at 0x7feda825be10>,\n",
       " <keras.layers.core.Activation at 0x7feda821af90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7feda8162ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7feda8194590>,\n",
       " <keras.layers.core.Activation at 0x7feda8194f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7feda8137fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7feda80986d0>,\n",
       " <keras.layers.core.Activation at 0x7feda8098890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7feda802e7d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed9077ef10>,\n",
       " <keras.layers.merge.Add at 0x7fed9077edd0>,\n",
       " <keras.layers.core.Activation at 0x7fed9072e850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed9063c750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed906a8450>,\n",
       " <keras.layers.core.Activation at 0x7fed906a8ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed905c8b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed9054fe50>,\n",
       " <keras.layers.core.Activation at 0x7fed905a6510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed904bf510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed904a8d90>,\n",
       " <keras.layers.merge.Add at 0x7fed9043db10>,\n",
       " <keras.layers.core.Activation at 0x7fed903da110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed90369690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed9033cc90>,\n",
       " <keras.layers.core.Activation at 0x7fed90356e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed902f1b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed901fadd0>,\n",
       " <keras.layers.core.Activation at 0x7fed90250490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed901bc250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed900dc450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed901aefd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed9007d790>,\n",
       " <keras.layers.merge.Add at 0x7fed707f37d0>,\n",
       " <keras.layers.core.Activation at 0x7fed7079a590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed7079a2d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed70735350>,\n",
       " <keras.layers.core.Activation at 0x7fed70719ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed706b7390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed70614c10>,\n",
       " <keras.layers.core.Activation at 0x7fed70614550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed70550bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed7052d410>,\n",
       " <keras.layers.merge.Add at 0x7fed7052df90>,\n",
       " <keras.layers.core.Activation at 0x7fed7044b790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed7044b850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed704353d0>,\n",
       " <keras.layers.core.Activation at 0x7fed70435e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed703632d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed7027d790>,\n",
       " <keras.layers.core.Activation at 0x7fed702c49d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed701fbd90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed701db090>,\n",
       " <keras.layers.merge.Add at 0x7fed701db510>,\n",
       " <keras.layers.core.Activation at 0x7fed70173f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed70173150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed700f1f10>,\n",
       " <keras.layers.core.Activation at 0x7fed700f1bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed587d1150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed587aeb10>,\n",
       " <keras.layers.core.Activation at 0x7fed586cec90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed587229d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed58644410>,\n",
       " <keras.layers.merge.Add at 0x7fed58644210>,\n",
       " <keras.layers.core.Activation at 0x7fed585e3910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed585e3710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed5855af90>,\n",
       " <keras.layers.core.Activation at 0x7fed5855add0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed5847d050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed58459a90>,\n",
       " <keras.layers.core.Activation at 0x7fed584593d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed58379f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed582b3dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed58373210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed5824aa50>,\n",
       " <keras.layers.merge.Add at 0x7fed58190310>,\n",
       " <keras.layers.core.Activation at 0x7fed581a8f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed580c4610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed58121490>,\n",
       " <keras.layers.core.Activation at 0x7fed58121f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed5803fb90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed3677dfd0>,\n",
       " <keras.layers.core.Activation at 0x7fed3679bc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed3671df50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed366a8190>,\n",
       " <keras.layers.merge.Add at 0x7fed36680690>,\n",
       " <keras.layers.core.Activation at 0x7fed36619e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed36619a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed36599310>,\n",
       " <keras.layers.core.Activation at 0x7fed365991d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed364d8990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed364ac090>,\n",
       " <keras.layers.core.Activation at 0x7fed364ac510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed363c6c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed36342210>,\n",
       " <keras.layers.merge.Add at 0x7fed3630d510>,\n",
       " <keras.layers.core.Activation at 0x7fed362ddcd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed362fc890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed36244b50>,\n",
       " <keras.layers.core.Activation at 0x7fed362446d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed361f6090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed361117d0>,\n",
       " <keras.layers.core.Activation at 0x7fed36158a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed360f5c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed3606d3d0>,\n",
       " <keras.layers.merge.Add at 0x7fed3606d6d0>,\n",
       " <keras.layers.core.Activation at 0x7fed35f88750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35f88810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35f04a90>,\n",
       " <keras.layers.core.Activation at 0x7fed35f04250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35e9fdd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35de7b50>,\n",
       " <keras.layers.core.Activation at 0x7fed35e1a8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35db7110>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35cd6810>,\n",
       " <keras.layers.merge.Add at 0x7fed35d1ba10>,\n",
       " <keras.layers.core.Activation at 0x7fed35c51bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35bd9390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35c31f90>,\n",
       " <keras.layers.core.Activation at 0x7fed35c31ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35b67d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35af1250>,\n",
       " <keras.layers.core.Activation at 0x7fed35acb890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35a64510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed359dd250>,\n",
       " <keras.layers.merge.Add at 0x7fed359dd110>,\n",
       " <keras.layers.core.Activation at 0x7fed3597b1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed3588d090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed358dfa90>,\n",
       " <keras.layers.core.Activation at 0x7fed358f3d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35811b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed357f8ed0>,\n",
       " <keras.layers.core.Activation at 0x7fed357f8d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed3572add0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed35628ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35691fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed355a0150>,\n",
       " <keras.layers.merge.Add at 0x7fed35545fd0>,\n",
       " <keras.layers.core.Activation at 0x7fed354c06d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed354c0410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed3543dbd0>,\n",
       " <keras.layers.core.Activation at 0x7fed3543d6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed353d3310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed3534fe90>,\n",
       " <keras.layers.core.Activation at 0x7fed3534f090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed352ea810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35264e10>,\n",
       " <keras.layers.merge.Add at 0x7fed3520be50>,\n",
       " <keras.layers.core.Activation at 0x7fed35188f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed351a1510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed35162f50>,\n",
       " <keras.layers.core.Activation at 0x7fed35162550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed3509e3d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed34fffe50>,\n",
       " <keras.layers.core.Activation at 0x7fed34fffad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fed34f9d250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fed34f17390>,\n",
       " <keras.layers.merge.Add at 0x7fed34f17190>,\n",
       " <keras.layers.core.Activation at 0x7fed34eb1b50>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fed35188bd0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fedaba11a10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 47/101 [============>.................] - ETA: 58:35 - loss: 0.2002 - acc: 0.9172"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
